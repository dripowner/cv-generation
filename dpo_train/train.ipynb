{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clearml\n",
    "from tqdm import tqdm\n",
    "from transformers import logging\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from util import read_jsonl, set_random_seed, fix_tokenizer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "отключаем wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чекпоинты с которых стартуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Train/models/sft_clean_only_2', '../Train/models/full-sft/full-sft']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"../Train/models/sft_clean_only_2\", \"../Train/models/full-sft/full-sft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"./data/all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_checkpoint = \"../Train/models/sft_clean_only_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = DPOConfig(\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    model_adapter_name=\"train\",\n",
    "    output_dir=\"./models/dpo_sft_clean_only\",\n",
    "    ref_adapter_name=\"reference\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=200,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args: DPOConfig,\n",
    "          sft_checkpoint):\n",
    "    logging.set_verbosity_info()\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(sft_checkpoint).to(\"cuda\")\n",
    "    model_ref = AutoModelForSeq2SeqLM.from_pretrained(sft_checkpoint).to(\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(sft_checkpoint, do_lower_case=False, strip_accents=False)\n",
    "    tokenizer = fix_tokenizer(tokenizer)\n",
    "\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model,\n",
    "        model_ref,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        args=args\n",
    "    )\n",
    "    dpo_trainer.train()\n",
    "    dpo_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: overwriting (reusing) task id=5702d94a055a4d7db83284b8500315a8\n",
      "2024-06-21 05:49:36,436 - clearml.Repository Detection - WARNING - Could not read Jupyter Notebook: No module named 'nbconvert'\n",
      "2024-06-21 05:49:36,453 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/4dfada0d47c54d358198351db8a35944/experiments/5702d94a055a4d7db83284b8500315a8/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../Train/models/sft_clean_only_2\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"../Train/models/sft_clean_only_2\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 200,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file ../Train/models/sft_clean_only_2\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ../Train/models/sft_clean_only_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ../Train/models/sft_clean_only_2\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading configuration file ../Train/models/sft_clean_only_2\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"../Train/models/sft_clean_only_2\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 200,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file ../Train/models/sft_clean_only_2\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ../Train/models/sft_clean_only_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ../Train/models/sft_clean_only_2\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "e:\\cv-generation\\train_venv\\Lib\\site-packages\\trl\\trainer\\dpo_trainer.py:363: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "e:\\cv-generation\\train_venv\\Lib\\site-packages\\trl\\trainer\\dpo_trainer.py:376: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "e:\\cv-generation\\train_venv\\Lib\\site-packages\\trl\\trainer\\dpo_trainer.py:389: UserWarning: When using an encoder decoder architecture, you should set `max_target_length` in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "e:\\cv-generation\\train_venv\\Lib\\site-packages\\trl\\trainer\\dpo_trainer.py:411: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  30000\n",
      "PAD:  0 <pad>\n",
      "BOS:  None None\n",
      "EOS:  1 </s>\n",
      "UNK:  2 <unk>\n",
      "SEP:  None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5665/5665 [00:03<00:00, 1845.98 examples/s]\n",
      "Map: 100%|██████████| 630/630 [00:00<00:00, 1799.46 examples/s]\n",
      "***** Running training *****\n",
      "  Num examples = 5,665\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,127\n",
      "  Number of trainable parameters = 244,309,248\n",
      "Automatic ClearML logging enabled.\n",
      "External ClearML Task has been connected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-21 05:49:52,526 - clearml.Task - WARNING - Parameters must be of builtin type (Transformers/accelerator_config[AcceleratorConfig])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2127 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  0%|          | 3/2127 [00:03<45:10,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.483, 'grad_norm': 3.1650643348693848, 'learning_rate': 4.992947813822285e-05, 'rewards/chosen': -0.24205994606018066, 'rewards/rejected': -1.4405349493026733, 'rewards/accuracies': 0.7083333134651184, 'rewards/margins': 1.1984750032424927, 'logps/rejected': -140.23036193847656, 'logps/chosen': -46.679996490478516, 'logits/rejected': -14.29877758026123, 'logits/chosen': -19.87135124206543, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2127 [00:13<1:43:33,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2987, 'grad_norm': 2.9279820919036865, 'learning_rate': 4.98589562764457e-05, 'rewards/chosen': -0.35415318608283997, 'rewards/rejected': -4.0842976570129395, 'rewards/accuracies': 0.7916666865348816, 'rewards/margins': 3.730144500732422, 'logps/rejected': -149.65765380859375, 'logps/chosen': -53.24116897583008, 'logits/rejected': -16.081806182861328, 'logits/chosen': -19.053930282592773, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/2127 [00:23<1:48:51,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3039, 'grad_norm': 2.7076990604400635, 'learning_rate': 4.978843441466855e-05, 'rewards/chosen': -0.627028226852417, 'rewards/rejected': -4.597595691680908, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9705677032470703, 'logps/rejected': -139.30250549316406, 'logps/chosen': -49.5007209777832, 'logits/rejected': -16.852684020996094, 'logits/chosen': -19.878767013549805, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/2127 [00:32<1:37:34,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1068, 'grad_norm': 0.0780976340174675, 'learning_rate': 4.97179125528914e-05, 'rewards/chosen': -0.7191200256347656, 'rewards/rejected': -8.514274597167969, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 7.795154094696045, 'logps/rejected': -239.5478515625, 'logps/chosen': -56.50431442260742, 'logits/rejected': -13.539361000061035, 'logits/chosen': -19.190202713012695, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/2127 [00:35<55:49,  1.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1483, 'grad_norm': 1.465187430381775, 'learning_rate': 4.964739069111425e-05, 'rewards/chosen': -0.9664502143859863, 'rewards/rejected': -8.065792083740234, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 7.099342346191406, 'logps/rejected': -223.7506866455078, 'logps/chosen': -56.21305465698242, 'logits/rejected': -14.275918006896973, 'logits/chosen': -21.04248809814453, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/2127 [00:38<41:38,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2515, 'grad_norm': 4.007912635803223, 'learning_rate': 4.9576868829337096e-05, 'rewards/chosen': -1.1411417722702026, 'rewards/rejected': -7.991770267486572, 'rewards/accuracies': 0.8333333134651184, 'rewards/margins': 6.8506293296813965, 'logps/rejected': -196.11610412597656, 'logps/chosen': -63.97511672973633, 'logits/rejected': -15.579852104187012, 'logits/chosen': -20.464513778686523, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2127 [00:40<38:15,  1.09s/it]***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "                                                 \n",
      "  1%|          | 20/2127 [00:51<38:15,  1.09s/it]Saving model checkpoint to ./models/dpo_sft_clean_only\\checkpoint-20\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'num_beams': 5}\n",
      "Configuration saved in ./models/dpo_sft_clean_only\\checkpoint-20\\config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Configuration saved in ./models/dpo_sft_clean_only\\checkpoint-20\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23409554362297058, 'eval_runtime': 11.5558, 'eval_samples_per_second': 54.518, 'eval_steps_per_second': 6.836, 'eval_rewards/chosen': -1.0006881952285767, 'eval_rewards/rejected': -7.152886867523193, 'eval_rewards/accuracies': 0.9018987417221069, 'eval_rewards/margins': 6.152198314666748, 'eval_logps/rejected': -184.58438110351562, 'eval_logps/chosen': -59.01625442504883, 'eval_logits/rejected': -15.373732566833496, 'eval_logits/chosen': -20.063398361206055, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./models/dpo_sft_clean_only\\checkpoint-20\\model.safetensors\n",
      "tokenizer config file saved in ./models/dpo_sft_clean_only\\checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./models/dpo_sft_clean_only\\checkpoint-20\\special_tokens_map.json\n",
      "  1%|          | 21/2127 [01:24<8:07:36, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0944, 'grad_norm': 0.9092710018157959, 'learning_rate': 4.9506346967559945e-05, 'rewards/chosen': -1.2179685831069946, 'rewards/rejected': -9.500365257263184, 'rewards/accuracies': 0.9583333134651184, 'rewards/margins': 8.282397270202637, 'logps/rejected': -232.39405822753906, 'logps/chosen': -53.48964309692383, 'logits/rejected': -13.429794311523438, 'logits/chosen': -19.407121658325195, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/2127 [01:29<3:26:57,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1655, 'grad_norm': 1.441392183303833, 'learning_rate': 4.9435825105782794e-05, 'rewards/chosen': -1.2342860698699951, 'rewards/rejected': -8.271882057189941, 'rewards/accuracies': 0.9583333134651184, 'rewards/margins': 7.037596225738525, 'logps/rejected': -206.01878356933594, 'logps/chosen': -48.2213020324707, 'logits/rejected': -14.40816879272461, 'logits/chosen': -21.194372177124023, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 27/2127 [01:33<1:39:16,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1676, 'grad_norm': 2.2706124782562256, 'learning_rate': 4.936530324400564e-05, 'rewards/chosen': -1.7660592794418335, 'rewards/rejected': -9.636094093322754, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 7.870035648345947, 'logps/rejected': -216.5866241455078, 'logps/chosen': -62.97658920288086, 'logits/rejected': -15.583358764648438, 'logits/chosen': -21.973798751831055, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 30/2127 [01:36<56:41,  1.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1004, 'grad_norm': 1.3310558795928955, 'learning_rate': 4.929478138222849e-05, 'rewards/chosen': -1.553697109222412, 'rewards/rejected': -10.005352973937988, 'rewards/accuracies': 1.0, 'rewards/margins': 8.451656341552734, 'logps/rejected': -227.3843536376953, 'logps/chosen': -67.5145034790039, 'logits/rejected': -13.367167472839355, 'logits/chosen': -19.333993911743164, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 33/2127 [01:40<47:03,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1685, 'grad_norm': 2.0915653705596924, 'learning_rate': 4.922425952045134e-05, 'rewards/chosen': -1.9473365545272827, 'rewards/rejected': -7.591085910797119, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 5.643749237060547, 'logps/rejected': -179.769775390625, 'logps/chosen': -64.32575225830078, 'logits/rejected': -16.235702514648438, 'logits/chosen': -22.289884567260742, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 36/2127 [01:43<46:47,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1501, 'grad_norm': 3.370659589767456, 'learning_rate': 4.915373765867419e-05, 'rewards/chosen': -2.6505696773529053, 'rewards/rejected': -9.779247283935547, 'rewards/accuracies': 0.9583333134651184, 'rewards/margins': 7.128677845001221, 'logps/rejected': -217.7071990966797, 'logps/chosen': -78.79676055908203, 'logits/rejected': -14.230456352233887, 'logits/chosen': -20.96211814880371, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 39/2127 [01:46<37:19,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1685, 'grad_norm': 2.8300859928131104, 'learning_rate': 4.908321579689704e-05, 'rewards/chosen': -3.5751054286956787, 'rewards/rejected': -10.389897346496582, 'rewards/accuracies': 0.9166666865348816, 'rewards/margins': 6.814791202545166, 'logps/rejected': -232.5690155029297, 'logps/chosen': -90.68653106689453, 'logits/rejected': -13.766007423400879, 'logits/chosen': -18.990304946899414, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 40/2127 [01:47<35:57,  1.03s/it]***** Running Evaluation *****\n",
      "  Num examples = 630\n",
      "  Batch size = 8\n",
      "                                                 \n",
      "  2%|▏         | 40/2127 [01:58<35:57,  1.03s/it]Saving model checkpoint to ./models/dpo_sft_clean_only\\checkpoint-40\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'num_beams': 5}\n",
      "Configuration saved in ./models/dpo_sft_clean_only\\checkpoint-40\\config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Configuration saved in ./models/dpo_sft_clean_only\\checkpoint-40\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22476530075073242, 'eval_runtime': 10.9485, 'eval_samples_per_second': 57.542, 'eval_steps_per_second': 7.216, 'eval_rewards/chosen': -3.591540813446045, 'eval_rewards/rejected': -9.646472930908203, 'eval_rewards/accuracies': 0.8987341523170471, 'eval_rewards/margins': 6.054932594299316, 'eval_logps/rejected': -209.5202178955078, 'eval_logps/chosen': -84.9247817993164, 'eval_logits/rejected': -15.45541763305664, 'eval_logits/chosen': -20.113487243652344, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./models/dpo_sft_clean_only\\checkpoint-40\\model.safetensors\n",
      "tokenizer config file saved in ./models/dpo_sft_clean_only\\checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./models/dpo_sft_clean_only\\checkpoint-40\\special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m task \u001b[38;5;241m=\u001b[39m clearml\u001b[38;5;241m.\u001b[39mTask\u001b[38;5;241m.\u001b[39minit(project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv-generation\u001b[39m\u001b[38;5;124m'\u001b[39m, task_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdpo-clean-only-test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msft_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m task\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, sft_checkpoint)\u001b[0m\n\u001b[0;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m fix_tokenizer(tokenizer)\n\u001b[0;32m     10\u001b[0m dpo_trainer \u001b[38;5;241m=\u001b[39m DPOTrainer(\n\u001b[0;32m     11\u001b[0m     model,\n\u001b[0;32m     12\u001b[0m     model_ref,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39margs\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m \u001b[43mdpo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m dpo_trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\transformers\\trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\transformers\\trainer.py:2732\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2729\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2732\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\transformers\\trainer.py:2815\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[0;32m   2814\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m-> 2815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2816\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n\u001b[0;32m   2817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_rng_state(output_dir)\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\transformers\\trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[1;34m(self, output_dir)\u001b[0m\n\u001b[0;32m   2920\u001b[0m     save_fsdp_optimizer(\n\u001b[0;32m   2921\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[0;32m   2922\u001b[0m     )\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m   2924\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[1;32m-> 2925\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2927\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   2929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[0;32m   2930\u001b[0m )\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\clearml\\binding\\frameworks\\__init__.py:34\u001b[0m, in \u001b[0;36m_patched_call.<locals>._inner_patch\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mpatched_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\clearml\\binding\\frameworks\\pytorch_bind.py:203\u001b[0m, in \u001b[0;36mPatchPyTorchModelIO._save\u001b[1;34m(original_fn, obj, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save\u001b[39m(original_fn, obj, f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 203\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;66;03m# if there is no main task or this is a nested call\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PatchPyTorchModelIO\u001b[38;5;241m.\u001b[39m_current_task:\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\torch\\serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 619\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\clearml\\binding\\frameworks\\__init__.py:30\u001b[0m, in \u001b[0;36m_patched_call.<locals>._inner_patch\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m ident \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39m_get_ident() \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY2 \u001b[38;5;28;01melse\u001b[39;00m threading\u001b[38;5;241m.\u001b[39mget_ident()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ident \u001b[38;5;129;01min\u001b[39;00m _recursion_guard:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m _recursion_guard[ident] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     32\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\torch\\serialization.py:853\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    852\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[1;32m--> 853\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(name, storage\u001b[38;5;241m.\u001b[39mdata_ptr(), num_bytes)\n",
      "File \u001b[1;32me:\\cv-generation\\train_venv\\Lib\\site-packages\\clearml\\task.py:4313\u001b[0m, in \u001b[0;36mTask.__register_at_exit.<locals>.ExitHooks.signal_handler\u001b[1;34m(self, sig, frame)\u001b[0m\n\u001b[0;32m   4310\u001b[0m \u001b[38;5;66;03m# if this is a sig term, we wait until __at_exit is called (basically do nothing)\u001b[39;00m\n\u001b[0;32m   4311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sig \u001b[38;5;241m==\u001b[39m signal\u001b[38;5;241m.\u001b[39mSIGINT:\n\u001b[0;32m   4312\u001b[0m     \u001b[38;5;66;03m# return original handler result\u001b[39;00m\n\u001b[1;32m-> 4313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m org_handler \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(org_handler) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43morg_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_recursion_protection_flag:\n\u001b[0;32m   4316\u001b[0m     \u001b[38;5;66;03m# call original\u001b[39;00m\n\u001b[0;32m   4317\u001b[0m     os\u001b[38;5;241m.\u001b[39mkill(os\u001b[38;5;241m.\u001b[39mgetpid(), sig)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-21 05:52:57,096 - clearml.Task - WARNING - ### TASK STOPPED - USER ABORTED - STATUS CHANGED ###\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "task = clearml.Task.init(project_name='cv-generation', task_name='dpo-clean-only-test')\n",
    "train(training_args, sft_checkpoint)\n",
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
